---
title: "Prediction Assignment"
author: "Sudipto Nandan"
date: "February 6, 2019"
output:
  word_document: default
  html_document:
    keep_md: yes
---

# Build a predictive model for the manner in which subjects did the exercise.

## Promblem Summary
To analyse the data colected using FuelBand, Fitbit (accelerometers) about subject's movement during different types of excercise. We need to build a model which can predict the type of "barball lifts""  ("classe") based on different movement patterns recorded by the subjects.

## Data
The training data for this project are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

## Processing

```{r echo=FALSE, warning=FALSE, message=FALSE, tidy=TRUE, results='hide'}
library(Rmisc)
library(ggplot2)
library(caret)
library(randomForest)
library(lattice)
library(plyr)

# Set seed for reproducibility
set.seed(13019)

trainingOrig <- read.csv("pml-training.csv")
testingOrig <- read.csv("pml-testing.csv")
```

```{R echo=FALSE,tidy=TRUE}
dim(trainingOrig)
```
Data contains 160 columns and 19622 rows. So we have ideally 160-1 predictors.  
We convert 2 columns with factors to number format - user_name, new_window
```{R echo=FALSE, warning=FALSE, message=FALSE, tidy=TRUE, results='hide'}
trainingOrig$user_name <- as.numeric(as.character(trainingOrig$user_name))
trainingOrig$new_window <- as.numeric(as.character(trainingOrig$new_window))
```

With head() function we saw many columns have NA. So we are removing all columns which has more than 10% NAs We have decided not to impute any Imputing data

```{R echo=FALSE, results='hide'}
NA_percent = 10
ColNA_percent <- nrow(trainingOrig) / 100 * NA_percent
removeColumns <- which(colSums(is.na(trainingOrig) | trainingOrig=="") > ColNA_percent)
trainingFinal <- trainingOrig[,-removeColumns]
testingFinal <- testingOrig[,-removeColumns]
```
```{R echo=FALSE,tidy=TRUE}
dim(trainingFinal)
```
Now we have 57 columns to chose the list of predictors from.

We have 3 columns related to time, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp and 
```{R echo=FALSE, results='hide'}
removeColumns <- grep("timestamp", names(trainingFinal))
trainingFinal <- trainingFinal[,-c(1, removeColumns )]
testingFinal <- testingFinal[,-c(1, removeColumns )]
```
```{R echo=FALSE,tidy=TRUE}
dim(trainingFinal)
```
After this we have 54 columns to chose the list of predictors from.

## Exploratory data analyses 

We divide our test set into testSet and ValidationSet

```{R echo=FALSE, results='hide'}
classeIndex <- which(names(trainingFinal) == "classe")
partition <- createDataPartition(y=trainingFinal$classe, p=0.75, list=FALSE)
trainSet <- trainingFinal[partition, ]
validationSet <- trainingFinal[-partition, ]
rbind(dim(trainSet),dim(validationSet))
```

What are some fields that have high correlations with the classe?
Let us check if any of the columns are related to each other, if so, we will remove them
```{R echo=FALSE, results='hide'}
# To huge to draw this
# featurePlot(x=trainSet[, -classeIndex],y=as.numeric(trainSet$classe),plot="pairs")
correlations <- cor(trainSet[, -classeIndex], as.numeric(trainSet$classe))
```
```{R echo=FALSE,tidy=TRUE}
tail(sort(correlations),5)
```

Top 2 correlations are -  magnet_arm_x, pitch_forearm

Using *Fig 1* We cannot see clearly any relationship with these 2 highly related columns. Probably we cannot select our columns by just seeing these columns correlation individually

## Model selection 

Let us try to identify variables with high correlations amongst each other in our set, so we can possibly exclude them from the pca or training.

Let us use PCA to reduce the number of columns
```{R echo=FALSE}
pcaPProc <- preProcess(trainSet[, -classeIndex], method = "pca", thresh = 0.99)
trainPCA <- predict(pcaPProc, trainSet[, -classeIndex])
dim(trainPCA)
validationPCA <- predict(pcaPProc, validationSet[, -classeIndex])
testPCA <- predict(pcaPProc, testingFinal[, -classeIndex])
```
We see that using PCA we could reduce the nunber of principal components to

Now, let us generate some Random Forest training.
We are using different number of trees 30,60,90,120 on testSet without PCA and testSet with PCA  and checking the accuracy.
This has been generated in *Table-1*


## Model selection
 
```{R echo=FALSE, results='hide'}
ntree <- 60
rfFinal <- randomForest(
    x=trainSet[, -classeIndex], 
    y=trainSet$classe,
    xtest=validationSet[, -classeIndex], 
    ytest=validationSet$classe, 
    ntree=ntree,
    keep.forest=TRUE,
    proximity=TRUE) #do.trace=TRUE

rfFinalPCA <- randomForest(
    x=trainPCA,
    y=trainSet$classe,
    xtest=validationPCA,
    ytest=validationSet$classe,
    ntree=ntree,
    keep.forest=TRUE,
    proximity=TRUE)
```

```{R echo=FALSE}
rfFinalTrainAcc <- round(1-sum(rfFinal$confusion[, 'class.error']),3)
print(paste0("Accuracy on trainsEt(Without PCA) with Trees- ",ntree," is: ",rfFinalTrainAcc))
rfFinalTestAcc <- round(1-sum(rfFinal$test$confusion[, 'class.error']),3)
print(paste0("Accuracy on testsEt(Without PCA) with Trees- ",ntree," is: ",rfFinalTestAcc))
rfFinal
```

Based on the list of accuracy, we can say that PCA is not helping us much in this, except for reducing
number of components. Also, with trees=60 gives us the best accuracy for test as well as train set. Increasing further is not increasing the accuracy.
So we select a model without PCA and with number of trees = 60

## Conclusion
 
This concludes that PCA is not helping us much in this, except for reducing
number of components. So we will select model without PCA and with number of trees = 60 which has a test accuracy of
0.989 and train accuracy of 0.982 and OOB estimate of  error rate: 0.34%

We draw some more plots in *Fig 2* for the final prediction to check chosen model

# Test results
 
Although we've chosen the `rfFinal` it's still nice to see what the model with PCA would predict on the final test set.
Let's look at predictions for all models on the final test set. 

```{R echo=FALSE}
cleaned=as.data.frame(predict(rfFinal, testingFinal), optional=TRUE)
pcaAll=as.data.frame(predict(rfFinalPCA, testPCA), optional=TRUE)
predictions <- t(cbind(
    cleaned=as.data.frame(predict(rfFinal, testingFinal), optional=TRUE),
    pcaAll=as.data.frame(predict(rfFinalPCA, testPCA), optional=TRUE)
))
predictions
```

The predictions don't really change a lot with each model, but since we have most faith in the `rfFinal`, we'll keep that as final answer. 

## Appendix 

### Table 1: Accuracy of different Random Forest Models with and without PCA (Number of trees - 30,60,90 & 120)
```{R echo=FALSE}
PCAAccuracyTable<-c()
for (ntree in seq(30, 120, by = 30))
{
rfFinal <- randomForest(
    x=trainSet[, -classeIndex], 
    y=trainSet$classe,
    xtest=validationSet[, -classeIndex], 
    ytest=validationSet$classe, 
    ntree=ntree,
    keep.forest=TRUE,
    proximity=TRUE)

rfFinalTrainAcc <- round(1-sum(rfFinal$confusion[, 'class.error']),3)
#print(paste0("Accuracy on trainsEt(Without PCA) with Trees- ",ntree," is: ",rfFinalTrainAcc))
rfFinalTestAcc <- round(1-sum(rfFinal$test$confusion[, 'class.error']),3)
#print(paste0("Accuracy on testsEt(Without PCA) with Trees- ",ntree," is: ",rfFinalTestAcc))

rfFinalPCA <- randomForest(
    x=trainPCA,
    y=trainSet$classe,
    xtest=validationPCA,
    ytest=validationSet$classe,
    ntree=ntree,
    keep.forest=TRUE,
    proximity=TRUE)
rfFinalPCATrainAcc <- round(1-sum(rfFinalPCA$confusion[, 'class.error']),3)
#print(paste0("Accuracy on trainsEt(with PCA) with Trees- ",ntree," is: ",rfFinalPCATrainAcc))
rfFinalPCATestAcc <- round(1-sum(rfFinalPCA$test$confusion[, 'class.error']),3)
#print(paste0("Accuracy on testsEt(with PCA) with Trees- ",ntree," is: ",rfFinalPCATestAcc))

PCAAccuracyTable <- rbind(PCAAccuracyTable,c(ntree,rfFinalTrainAcc,rfFinalTestAcc,rfFinalPCATrainAcc,rfFinalPCATestAcc))
}
colnames(PCAAccuracyTable) <- c("No. tress","Test(No PCA)", "Train(Non PCA)","Test(PCA)", "Train(PCA)")
PCAAccuracyTable
```

### Figure 1: Density plot of these columns with "classe" column
```{R echo=FALSE}
p1 <- qplot(pitch_forearm,color=classe,data=trainSet,geom="density")
p2 <- qplot(magnet_arm_x,color=classe,data=trainSet,geom="density")
multiplot(p1,p2,cols=2)
```

### Figure 2: Variable Importance Plot for the Final Model and Error Vs No of Trees Plot
```{R echo=FALSE}
par(mfrow=c(1,2)) 
varImpPlot(rfFinal, cex=0.7, pch=16, main='Variable Importance Plot for Final Model')
plot(rfFinal,cex=0.7, main='Error vs No. of trees plot')
par(mfrow=c(1,1)) 
```

